{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uczenie maszynowe: Lab3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectFromModel, SelectKBest, f_classif\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analiza modelu klasyfikacyjnego\n",
    "* wziąć konkretne dane, podzielić na część treningową i testową lub uczyć wg. schematu walidacji\n",
    "krzyżowej\n",
    "* sprawdzić jakie są hiperparametry danego modelu i znaleźć optymalne\n",
    "* uczyć parametry modelu z danych, po skończonej nauce je wypisać"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Walidacja krzyżowa\n",
    "Techniką używana do oceny skuteczności modelu uczenia maszynowego poprzez podzielenie danych na wiele podzbiorów (foldów). Pozwala na uzyskanie bardziej wiarygodnych ocen wydajności modelu, minimalizując ryzyko overfittingu oraz underfittingu\n",
    "- Overfitting: model zbyt dobrze dopasowany do danych treningowych, ale słabo generalizujący (model jest zbyt skomplikowany i ma zbyt wiele parametrów w stosunku do liczby dostępnych danych)\n",
    "- Underfitting: model słabo dopasowany do danych treningowych, słabo generalizujący (model jest zbyt prosty, aby uchwycić podstawowe wzorce w danych)\n",
    "- Uśrednianie wyników: w kontekście walidacji krzyżowej oznacza, że po przeprowadzeniu wielu iteracji trenowania i testowania modelu na różnych podzbiorach danych, wyniki uzyskane w każdej iteracji są łączone i obliczana jest ich średnia. Proces ten zapewnia bardziej stabilną i wiarygodną ocenę wydajności modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X_test =  (360, 64)\n",
      "Y_test =  (360,)\n",
      "X_train =  (1437, 64)\n",
      "Y_train =  (1437,)\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore', category=ConvergenceWarning) # ignorowanie ostrzeżeń o braku zbieżności\n",
    "\n",
    "data = load_digits()\n",
    "\n",
    "X = data.data # features (cechy) -> używane do trenowania modelu\n",
    "y = data.target # etykiety -> wartości, które chcemy przewidzieć\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # test_size -> 80% training and 20% test\n",
    "# random_state = 42 (służy jako ziarne losowości, aby wyniki były powtarzalne), natomiast jak wartość jest ta sama za każdym razem\n",
    "# to mamy ten sam podział\n",
    "\n",
    "# dane testowe \n",
    "print (\"\\nX_test = \", X_test.shape) # (próbki, cechy)\n",
    "print (\"Y_test = \", y_test.shape,) # (próbki, )\n",
    "# dane treningowe \n",
    "print (\"X_train = \", X_train.shape)\n",
    "print (\"Y_train = \", y_train.shape)\n",
    "\n",
    "# Walidacja krzyżowa k - fold\n",
    "# Dzielimy na k równych podzbiorów (w których każdy z podzbiorów raz występuje jako zbiór uczący, a pozostała, połączona \n",
    "# część zbioru jest wykorzystywana jako zbiór testowy)\n",
    "\n",
    "# czyli dla k = 5, mamy 5 iteracji gdzie będziemy mieli podział na (w każdej iteracji) np. 4/5 zbioru treningowego i 1/5 zbioru testowego\n",
    "# na koniec wyniki są uśredniane\n",
    "\n",
    "k = 5\n",
    "cv = StratifiedKFold(n_splits=k) # -> schemat walidacji krzyżowej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(X_train, X_test, Y_train, Y_test, model, param_grid, cv):\n",
    "    cross_val_scores = cross_val_score(model, X_train, Y_train, cv=cv, scoring='accuracy') # metryka oceny -> dokładność\n",
    "    # zwraca wyniki walidacji krzyżowej dla każdej iteracji\n",
    "\n",
    "    # Walidacja krzyżowa - technika trenowania i testowania modelu na różnych podzbiorach danych\n",
    "    # Dostajemy procentowo jak dobrze model działa tzn jaka jest dokładność\n",
    "    # na danym podzbiorze danych\n",
    "    print(\"\\nWyniki walidacji krzyżowej: \\n\", cross_val_scores)\n",
    "    print(\"Średni wynik walidacji krzyżowej: \", cross_val_scores.mean()) \n",
    "\n",
    "    # Grid Search -> wyszukanie najlepszych hiperparametrów\n",
    "    # GridSearchCV przeprowadza przeszukiwanie siatki hiperparametrów na danych treningowych.\n",
    "    # estimator=model określa, który model jest trenowany.\n",
    "    # param_grid zawiera siatkę hiperparametrów do przetestowania\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=cv, scoring='accuracy')\n",
    "    grid_search.fit(X_train, Y_train)\n",
    "\n",
    "    # Najlepsze hiperparametry\n",
    "    best_params = grid_search.best_params_\n",
    "    print(f\"Najlepsze hiperparametry: {best_params}\")\n",
    "\n",
    "    # Trenowanie modelu z najlepszymi hiperparametrami\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_model.fit(X_train, Y_train) # trenowanie modelu z najlepszymi hiperparametrami\n",
    "\n",
    "    # Predykcje i ocena na zbiorze testowym\n",
    "    y_pred = best_model.predict(X_test) # Model z najlepszymi hiperparametrami dokonuje predykcji na zbiorze testowym\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Dokładność na zbiorze testowym: {accuracy:.4f}\")\n",
    "    print(\"Raport klasyfikacji:\\n\", classification_report(y_test, y_pred, zero_division=0)) \n",
    "    # raport klaasyfikacji -> precision, recall, f1-score\n",
    "\n",
    "    # Wypisanie końcowych parametrów modelu\n",
    "    print(\"Końcowe parametry modelu:\")\n",
    "    print(best_model.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Naiwny klasyfikator Bayesa\n",
    "Prosty, ale potężny algorytm uczenia maszynowego stosowany głównie do problemów klasyfikacji. Nazwa \"naiwny\" pochodzi od założenia, że wszystkie cechy (atrybuty) w zbiorze danych są niezależne od siebie, co rzadko jest prawdą w rzeczywistych zastosowaniach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianNB()\n",
    "param_grid = {\n",
    "    'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5]  # Dodaje niewielką wartość do wariancji, aby zapobiec dzieleniu przez zero\n",
    "}\n",
    "\n",
    "evaluate_model(X_train, X_test, y_train, y_test, model, param_grid, cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model kNN (model k najbliższych sąsiadów)\n",
    "Ma za zadnie znaleźć k sąsiadów, do których klasyfikowane obiekty są najbliższe dla wybranej metryki (np. Euklidesa), a następnie określa\n",
    "wynik klasyfikacji na podstawie większości głosów tych najbliższych k sąsiadów, biorąc pod uwagę, która klasa jest reprezentowana największą liczbę razy w grupie k najbliższych sąsiadów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier()\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9], # Liczba najbliższych sąsiadów\n",
    "    'weights': ['uniform', 'distance'], # Waga dla sąsiadów, uniform - wszyscy sąsiedzi mają taką samą wagę, distance - im bliżej tym większa waga\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'], # Algorytm używany do wyszukiwania najbliższych sąsiadów.\n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski'] # Metryka używana do obliczania odległości\n",
    "}\n",
    "\n",
    "# Poprawia wydajność modeli wrażliwych na skalowanie danych\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train) # oblicza średnią i odchylenie standardowe na zbiorze treningowym i przekształca dane treningowe\n",
    "# tak aby miały średnią 0 i wariancję 1\n",
    "X_test = scaler.transform(X_test) # przekształca dane testowe\n",
    "\n",
    "evaluate_model(X_train, X_test, y_train, y_test, model, param_grid, cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Drzewo decyzyjne\n",
    "Model drzewa decyzyjnego jest strukturą drzewa, gdzie węzły reprezentują decyzje na podstawie wartości cech, a gałęzie przedstawiają możliwe wyniki tych decyzji. Liście drzewa reprezentują klasy (w przypadku klasyfikacji) lub wartości przewidywane (w przypadku regresji).\n",
    "\n",
    "#### Jak działa drzewo decyzyjne?\n",
    "##### Budowanie drzewa:\n",
    "\n",
    "- Algorytm zaczyna od całego zbioru danych jako korzenia drzewa.\n",
    "- W każdym węźle wybiera najlepszą cechę i wartość progu do podziału danych, maksymalizując czystość podzbiorów po podziale.\n",
    "- Proces ten jest rekurencyjnie powtarzany dla każdego podzbioru, tworząc gałęzie drzewa, aż do spełnienia kryterium zatrzymania (np. maksymalna głębokość drzewa, minimalna liczba próbek w liściu).\n",
    "Czystość podzbiorów:\n",
    "\n",
    "#### Czystość podzbiorów \n",
    "- Po podziale jest oceniana za pomocą miar takich jak indeks Giniego czy entropia. Celem jest maksymalizacja czystości podzbiorów, tj. dążenie do sytuacji, gdzie podzbiory zawierają próbki głównie jednej klasy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'], # Kryterium oceny jakości podziału. indeks Giniego/Entropia (używa entropii do oceny czystości podziału)\n",
    "    'max_depth': [None, 10, 20, 30], # Maksymalna głębokość drzewa, None - brak ograniczenia\n",
    "    'min_samples_split': [2, 5, 10], # Minimalna liczba próbek wymagana do podziału węzła\n",
    "    'min_samples_leaf': [1, 2, 4], # Minimalna liczba próbek wymagana w liściu węzła\n",
    "    'splitter': ['best', 'random'], # Strategia podziału węzła 'best' - najlepszy podział, 'random' - losowy podział\n",
    "    'max_features': ['sqrt', 'log2'], # Maksymalna liczba cech branych pod uwagę przy poszukiwaniu najlepszego podziału\n",
    "}\n",
    "\n",
    "evaluate_model(X_train, X_test, y_train, y_test, model, param_grid, cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Regresja logistyczna\n",
    "Algorytmem klasyfikacyjnym, który jest używany do przewidywania binarnych wyników (0 lub 1). Model ten oblicza prawdopodobieństwo przynależności próbki do określonej klasy, stosując funkcję logistyczną (sigmoidę)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(max_iter=1000) # max_iter - maksymalna liczba iteracji\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100], # Odwrotność siły regularyzacji, mniejsza wartość oznacza silniejszą regularyzację\n",
    "    'solver': ['liblinear', 'saga'], # Algorytm używany do optymalizacji\n",
    "    # 'penalty': ['l1', 'l2', 'elasticnet'], # Rodzaj regularyzacji (None - break regularyzacji)\n",
    "}\n",
    "\n",
    "# Poprawia wydajność modeli wrażliwych na skalowanie danych\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "evaluate_model(X_train, X_test, y_train, y_test, model, param_grid, cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Model LDA (Linear Discriminant Analysis)\n",
    "Jest to technika stosowana w statystyce, rozpoznawaniu wzorców i uczeniu maszynowym do znajdowania liniowej kombinacji cech, która najlepiej oddziela dwie lub więcej klas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearDiscriminantAnalysis()\n",
    "param_grid = [\n",
    "    {'solver': ['svd']},\n",
    "    {'solver': ['lsqr', 'eigen'], 'shrinkage': ['auto', 0.1, 0.5, 1.0]}\n",
    "]\n",
    "\n",
    "evaluate_model(X_train, X_test, y_train, y_test, model, param_grid, cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Model MLP (Prosta sieć neuronowa)\n",
    "Jest rodzajem sztucznej sieci neuronowej stosowanej w uczeniu nadzorowanym do rozwiązywania problemów klasyfikacyjnych. Jest to pełnoprawny klasyfikator, który uczy się na podstawie danych wejściowych i dostarcza predykcje klas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPClassifier(max_iter=100)\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 50)], # Wiele warstw ukrytych\n",
    "    'activation': ['tanh', 'relu'], # Funkcja aktywacji dla warstw ukrytych\n",
    "    'solver': ['sgd', 'adam'], # Optymalizator\n",
    "    'alpha': [0.0001, 0.1], # Parametr regularyzacji (L2)\n",
    "    'learning_rate': ['constant', 'adaptive'], # Plan uczenia\n",
    "}\n",
    "\n",
    "# Poprawia wydajność modeli wrażliwych na skalowanie danych\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "evaluate_model(X_train, X_test, y_train, y_test, model, param_grid, cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Model SVM (Support Vector Machine)\n",
    "Algorytm uczenia maszynowego używany głównie do zadań klasyfikacyjnych, ale może być również stosowany do regresji (Support Vector Regression). SVM znajduje hiperpłaszczyznę, która najlepiej rozdziela dane w przestrzeni cech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC()\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100], # Parametr regularyzacji. Kontroluje kompromis między maksymalizacją marginesu a minimalizacją błędu klasyfikacji.\n",
    "    'gamma': [1, 0.1, 0.01, 0.001], # Parametr funkcji jądrowej RBF (Radial Basis Function). \n",
    "    # Wyższa wartość oznacza większy wpływ próbek treningowych.\n",
    "    'kernel': ['linear', 'rbf'] # Funkcja jądrowa\n",
    "}\n",
    "\n",
    "# Poprawia wydajność modeli wrażliwych na skalowanie danych\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "evaluate_model(X_train, X_test, y_train, y_test, model, param_grid, cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Bagging\n",
    "Metoda zespołowa (ensemble), która poprawia stabilność i dokładność algorytmów uczenia maszynowego poprzez zmniejszenie wariancji. Metoda polega na trenowaniu wielu modeli bazowych na różnych losowo wybranych podzbiorach danych treningowych, a następnie łączeniu ich predykcji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = DecisionTreeClassifier()\n",
    "model = BaggingClassifier(estimator=base_model, random_state=42) # defaultowo estimator to DecisionTreeClassifier\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 10], # Liczba modeli bazowych (drzew decyzyjnych) w ensemble.\n",
    "    'max_features': [0.5, 0.7, 1.0], # Maksymalna liczba cech branych pod uwagę przy poszukiwaniu najlepszego podziału\n",
    "    'max_samples': [0.5, 1.0], # Liczba próbek używanych do trenowania każdego modelu bazowego\n",
    "    'bootstrap': [True, False], # Czy próbki są losowane z powtórzeniami\n",
    "    'bootstrap_features': [True, False], # Czy cechy są losowane z powtórzeniami\n",
    "}\n",
    "\n",
    "# Poprawia wydajność modeli wrażliwych na skalowanie danych\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "evaluate_model(X_train, X_test, y_train, y_test, model, param_grid, cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Boosting\n",
    "Technika zespołowa, która tworzy silny model predykcyjny poprzez łączenie słabych modeli bazowych, zwykle drzew decyzyjnych. Metoda ta działa iteracyjnie, poprawiając błędy poprzednich modeli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingClassifier()\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200], # Liczba drzew decyzyjnych\n",
    "    'learning_rate': [0.01, 0.1], # Współczynnik uczenia\n",
    "    'max_depth': [3, 5, 7], # Maksymalna głębokość drzewa\n",
    "}\n",
    "\n",
    "# Poprawia wydajność modeli wrażliwych na skalowanie danych\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "evaluate_model(X_train, X_test, y_train, y_test, model, param_grid, cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Lasy losowe\n",
    "Metoda zespołowa (ensemble) polegająca na budowie wielu drzew decyzyjnych podczas treningu i wyjściu klasy będącej trybem klas (dla klasyfikacji) lub średnią predykcji (dla regresji) poszczególnych drzew. Random forest łączy prostotę drzew decyzyjnych z możliwościami agregacji w celu uzyskania lepszej wydajności i stabilności predykcji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(random_state=42)\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],  # Liczba drzew w lesie\n",
    "    'max_features': ['sqrt', 'log2'],  # Liczba cech do rozważenia przy podziale\n",
    "    'max_depth': [None, 10, 20, 30],  # Maksymalna głębokość drzew\n",
    "    'min_samples_split': [2, 5],  # Minimalna liczba próbek potrzebnych do podziału węzła\n",
    "    'min_samples_leaf': [1, 2, 4],  # Minimalna liczba próbek w liściu\n",
    "    'bootstrap': [True, False]  # Czy losować próbki z powtórzeniami\n",
    "}\n",
    "\n",
    "# Poprawia wydajność modeli wrażliwych na skalowanie danych\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "evaluate_model(X_train, X_test, y_train, y_test, model, param_grid, cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uczenie nienadzorowane\n",
    "To jeden z głównych typów uczenia maszynowego, gdzie model jest trenowany na danych, które nie mają zdefiniowanych etykiet. Innymi słowy, nie dostarczamy modelowi przykładów, które wskazują, jakie powinny być wyniki (brak z góry określonych odpowiedzi). Celem uczenia nienadzorowanego jest odkrycie ukrytych struktur, wzorców lub zależności w danych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Klasteryzacja (różne algorytmy)\n",
    "Technika uczenia maszynowego, która polega na grupowaniu danych w taki sposób, aby obiekty w tych samych grupach (klastrach) były do siebie bardziej podobne niż obiekty w różnych grupach. Jest to metoda uczenia nienadzorowanego, co oznacza, że model nie korzysta z etykiet ani kategorii podczas procesu grupowania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Generowanie danych 1D za pomocą make_blobs\n",
    "def generate_1d_data():\n",
    "    X, _ = make_blobs(n_samples=300, centers=[[-2], [0], [2]], cluster_std=0.5, random_state=0)\n",
    "    return X\n",
    "\n",
    "# Generowanie danych 2D za pomocą make_blobs\n",
    "def generate_2d_data():\n",
    "    X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n",
    "    return X\n",
    "\n",
    "# KMeans klasteryzacja\n",
    "def kmeans_clustering(X, n_clusters=3):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)  # Tworzenie instancji modelu KMeans\n",
    "    labels = kmeans.fit_predict(X)  # Dopasowanie modelu i przewidywanie klastrów\n",
    "    score = silhouette_score(X, labels)  # Obliczanie wskaźnika Silhouette\n",
    "    print(f\"KMeans Silhouette Score: {score:.4f}\")  # Wyświetlanie wskaźnika Silhouette\n",
    "    return labels, kmeans.cluster_centers_  # Zwracanie etykiet klastrów i centrów klastrów\n",
    "\n",
    "# Hierarchical Clustering\n",
    "def hierarchical_clustering(X, n_clusters=3):\n",
    "    hierarchical = AgglomerativeClustering(n_clusters=n_clusters)  # Tworzenie instancji modelu Hierarchical Clustering\n",
    "    labels = hierarchical.fit_predict(X)  # Dopasowanie modelu i przewidywanie klastrów\n",
    "    score = silhouette_score(X, labels)  # Obliczanie wskaźnika Silhouette\n",
    "    print(f\"Hierarchical Clustering Silhouette Score: {score:.4f}\")  # Wyświetlanie wskaźnika Silhouette\n",
    "    return labels  # Zwracanie etykiet klastrów\n",
    "\n",
    "# Wizualizacja klasteryzacji dla danych 1D\n",
    "def plot_1d_clustering(X, labels, centers, title):\n",
    "    plt.scatter(X, np.zeros_like(X), c=labels, cmap='viridis', s=50)  # Rysowanie punktów danych\n",
    "    if centers is not None:\n",
    "        plt.scatter(centers, np.zeros_like(centers), c='red', s=200, alpha=0.75)  # Rysowanie centrów klastrów\n",
    "    plt.title(title)  # Tytuł wykresu\n",
    "    plt.xlabel('Value')  # Etykieta osi X\n",
    "    plt.show()  # Wyświetlanie wykresu\n",
    "\n",
    "# Wizualizacja klasteryzacji dla danych 2D\n",
    "def plot_2d_clustering(X, labels, centers, title):\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=50)  # Rysowanie punktów danych\n",
    "    if centers is not None:\n",
    "        plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.75)  # Rysowanie centrów klastrów\n",
    "    plt.title(title)  # Tytuł wykresu\n",
    "    plt.xlabel('Feature 1')  # Etykieta osi X\n",
    "    plt.ylabel('Feature 2')  # Etykieta osi Y\n",
    "    plt.show()  # Wyświetlanie wykresu\n",
    "\n",
    "# Klasteryzacja i wizualizacja dla danych 1D\n",
    "def clustering_1d():\n",
    "    X = generate_1d_data()  # Generowanie danych 1D\n",
    "\n",
    "    # KMeans\n",
    "    labels, centers = kmeans_clustering(X, n_clusters=3)  # Klasteryzacja KMeans\n",
    "    plot_1d_clustering(X, labels, centers, 'KMeans Clustering (1D)')  # Wizualizacja wyników KMeans\n",
    "\n",
    "    # Hierarchical Clustering\n",
    "    labels = hierarchical_clustering(X, n_clusters=3)  # Klasteryzacja Hierarchical Clustering\n",
    "    plot_1d_clustering(X, labels, None, 'Hierarchical Clustering (1D)')  # Wizualizacja wyników Hierarchical Clustering\n",
    "\n",
    "# Klasteryzacja i wizualizacja dla danych 2D\n",
    "def clustering_2d():\n",
    "    X = generate_2d_data()  # Generowanie danych 2D\n",
    "\n",
    "    # KMeans\n",
    "    labels, centers = kmeans_clustering(X, n_clusters=4)  # Klasteryzacja KMeans\n",
    "    plot_2d_clustering(X, labels, centers, 'KMeans Clustering (2D)')  # Wizualizacja wyników KMeans\n",
    "\n",
    "    # Hierarchical Clustering\n",
    "    labels = hierarchical_clustering(X, n_clusters=4)  # Klasteryzacja Hierarchical Clustering\n",
    "    plot_2d_clustering(X, labels, None, 'Hierarchical Clustering (2D)')  # Wizualizacja wyników Hierarchical Clustering\n",
    "\n",
    "# Uruchomienie klasteryzacji\n",
    "clustering_1d()  # Klasteryzacja i wizualizacja dla danych 1D\n",
    "clustering_2d()  # Klasteryzacja i wizualizacja dla danych 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Szacowanie gęstości rozkładu (kernel density estimation)\n",
    "Kernel Density Estimation (KDE) jest zaawansowaną metodą, która pozwala na gładkie i dokładne oszacowanie gęstości rozkładu danych, w przeciwieństwie do bardziej podstawowych metod, takich jak histogramy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Szacowanie gęstości rozkładu w 1D\n",
    "def kde_1d():\n",
    "    # Generowanie danych 1D za pomocą make_blobs\n",
    "    X, _ = make_blobs(n_samples=300, centers=[[-2], [0], [2]], cluster_std=0.5, random_state=0)\n",
    "\n",
    "    # Definicja siatki hiperparametrów\n",
    "    param_grid = {\n",
    "        'bandwidth': np.linspace(0.1, 1.0, 20),  # Zmniejszenie liczby punktów w siatce\n",
    "    }\n",
    "\n",
    "    # Tworzenie modelu KDE\n",
    "    kde = KernelDensity()\n",
    "\n",
    "    # Dopasowanie modelu GridSearchCV do znalezienia najlepszych hiperparametrów\n",
    "    grid_search = GridSearchCV(kde, param_grid, cv=5)\n",
    "    grid_search.fit(X)\n",
    "\n",
    "    # Najlepsze hiperparametry\n",
    "    best_params = grid_search.best_params_\n",
    "    print(f\"Najlepsze hiperparametry (1D): {best_params}\")\n",
    "\n",
    "    # Dopasowanie modelu KDE z najlepszymi hiperparametrami\n",
    "    kde = KernelDensity(**best_params)\n",
    "    kde.fit(X)\n",
    "\n",
    "    # Generowanie próbek\n",
    "    X_d = np.linspace(-4, 4, 1000)[:, np.newaxis]\n",
    "    log_dens = kde.score_samples(X_d)\n",
    "\n",
    "    # Wizualizacja szacowania gęstości\n",
    "    plt.fill_between(X_d[:, 0], np.exp(log_dens), alpha=0.5)\n",
    "    plt.plot(X[:, 0], np.full_like(X[:, 0], -0.01), '|k', markeredgewidth=1)\n",
    "    plt.title(\"Szacowanie gęstości rozkładu (KDE) - dane 1D\")\n",
    "    plt.xlabel(\"Wartości\")\n",
    "    plt.ylabel(\"Gęstość\")\n",
    "    plt.show()\n",
    "\n",
    "# Szacowanie gęstości rozkładu w 2D\n",
    "def kde_2d():\n",
    "    # Generowanie danych 2D\n",
    "    X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n",
    "\n",
    "    # Definicja siatki hiperparametrów\n",
    "    param_grid = {\n",
    "        'bandwidth': np.linspace(0.1, 1.0, 20),  # Zmniejszenie liczby punktów w siatce\n",
    "    }\n",
    "\n",
    "    # Tworzenie modelu KDE\n",
    "    kde = KernelDensity()\n",
    "\n",
    "    # Dopasowanie modelu GridSearchCV do znalezienia najlepszych hiperparametrów\n",
    "    grid_search = GridSearchCV(kde, param_grid, cv=5)\n",
    "    grid_search.fit(X)\n",
    "\n",
    "    # Najlepsze hiperparametry\n",
    "    best_params = grid_search.best_params_\n",
    "    print(f\"Najlepsze hiperparametry (2D): {best_params}\")\n",
    "\n",
    "    # Dopasowanie modelu KDE z najlepszymi hiperparametrami\n",
    "    kde = KernelDensity(**best_params)\n",
    "    kde.fit(X)\n",
    "\n",
    "    # Generowanie siatki punktów do wizualizacji\n",
    "    x = np.linspace(X[:, 0].min() - 1, X[:, 0].max() + 1, 100)\n",
    "    y = np.linspace(X[:, 1].min() - 1, X[:, 1].max() + 1, 100)\n",
    "    X_grid, Y_grid = np.meshgrid(x, y)\n",
    "    grid_points = np.vstack([X_grid.ravel(), Y_grid.ravel()]).T\n",
    "\n",
    "    # Obliczanie gęstości na siatce punktów\n",
    "    log_dens = kde.score_samples(grid_points)\n",
    "    dens = np.exp(log_dens).reshape(X_grid.shape)\n",
    "\n",
    "    # Wizualizacja szacowania gęstości\n",
    "    plt.contourf(X_grid, Y_grid, dens, cmap='Blues')\n",
    "    plt.scatter(X[:, 0], X[:, 1], c='red', s=5)\n",
    "    plt.title(\"Szacowanie gęstości rozkładu (KDE) - dane 2D\")\n",
    "    plt.xlabel(\"Feature 1\")\n",
    "    plt.ylabel(\"Feature 2\")\n",
    "    plt.show()\n",
    "\n",
    "# Wykonanie funkcji\n",
    "kde_1d()\n",
    "kde_2d()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inżynieria cech\n",
    "Inżynieria cech to proces przekształcania danych w celu poprawy wydajności modeli uczenia maszynowego. Obejmuje to zarówno redukcję wymiarowości (np. metodą PCA), jak i selekcję cech, aby wybrać najbardziej informacyjne cechy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selekcja cech (różne algorytmy).\n",
    "Selekcja cech polega na wyborze podzbioru cech, które są najbardziej istotne dla danego problemu predykcyjnego. Można to zrobić na różne sposoby, np. na podstawie znaczenia cech określonego przez model lub poprzez univariate selection, gdzie każda cecha jest oceniana indywidualnie.\n",
    "- Celem jest zredukowanie liczby cech do tych, które są najbardziej informatywne i istotne dla modelu, co może prowadzić do poprawy jego wydajności, uproszczenia modelu i zmniejszenia ryzyka przeuczenia (overfitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
    "# Dane\n",
    "data = load_digits()\n",
    "\n",
    "X = data.data \n",
    "y = data.target \n",
    "\n",
    "# Trenowanie modelu\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standaryzacja danych\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Tworzenie modelu Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Trening modelu bez selekcji cech\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Dokładność na zbiorze testowym (bez selekcji): {accuracy:.4f}\")\n",
    "print(\"Raport klasyfikacji (bez selekcji):\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Selekcja cech na podstawie znaczenia cech\n",
    "selector = SelectFromModel(rf, prefit=True) # wybiera cechy, które mają znaczenie # prefit = True, ponieważ model jest już wytrenowany\n",
    "X_train_rf = selector.transform(X_train)\n",
    "X_test_rf = selector.transform(X_test)\n",
    "\n",
    "# Trening modelu z selekcją cech (znaczenie cech)\n",
    "model.fit(X_train_rf, y_train)\n",
    "y_pred_rf = model.predict(X_test_rf)\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"Dokładność na zbiorze testowym (znaczenie cech): {accuracy_rf:.4f}\")\n",
    "print(\"Raport klasyfikacji (znaczenie cech):\\n\", classification_report(y_test, y_pred_rf))\n",
    "\n",
    "# Wybieranie najlepszych cech za pomocą testu ANOVA\n",
    "# k - liczba cech do wybrania\n",
    "# score_func - funkcja oceny cech\n",
    "selector = SelectKBest(score_func=f_classif, k=2) #  wybiera k cech, które mają najwyższe wyniki dla wybranego testu statystycznego, \n",
    "# mierząc ich zależność z docelową zmienną\n",
    "X_train_best = selector.fit_transform(X_train, y_train)\n",
    "X_test_best = selector.transform(X_test)\n",
    "\n",
    "# Trenowanie modelu po selekcji cech\n",
    "model.fit(X_train_best, y_train)\n",
    "y_pred_best = model.predict(X_test_best)\n",
    "accuracy_best = accuracy_score(y_test, y_pred_best)\n",
    "print(f\"Dokładność na zbiorze testowym (Univariate Selection): {accuracy_best:.4f}\")\n",
    "print(\"Raport klasyfikacji (Univariate Selection):\\n\", classification_report(y_test, y_pred_best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redukcja wymiarowości: Metoda PCA\n",
    "Principal Component Analysis (PCA) to statystyczna metoda analizy danych, która przekształca dane o wysokiej wymiarowości na dane o mniejszej liczbie wymiarów. Celem PCA jest zredukowanie liczby wymiarów przy jednoczesnym zachowaniu jak największej ilości informacji zawartej w oryginalnych danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wczytywanie danych\n",
    "data = load_digits()\n",
    "\n",
    "X = data.data \n",
    "y = data.target \n",
    "\n",
    "# Podział danych na zbiór treningowy i testowy\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standaryzacja danych -> każda zmienna ma średnią 0 i wariancję 1 aby zmienne występujące w zbiorze danych były tej samej skali\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Trenowanie modelu bez redukcji wymiarowości\n",
    "# 100 drzew decyzyjnych\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Dokładność na zbiorze testowym (bez redukcji): {accuracy:.4f}\")\n",
    "print(\"Raport klasyfikacji (bez redukcji):\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Redukcja wymiarowości za pomocą PCA\n",
    "pca = PCA(n_components=2) # redukuje do 2 składowych głównych\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "model.fit(X_train_pca, y_train)\n",
    "y_pred_pca = model.predict(X_test_pca)\n",
    "accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
    "print(f\"\\nDokładność na zbiorze testowym (PCA): {accuracy_pca:.4f}\")\n",
    "print(\"Raport klasyfikacji (PCA):\\n\", classification_report(y_test, y_pred_pca))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
