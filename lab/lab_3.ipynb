{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uczenie maszynowe: Lab3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectFromModel, SelectKBest, f_classif\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analiza modelu klasyfikacyjnego\n",
    "* wziąć konkretne dane, podzielić na część treningową i testową lub uczyć wg. schematu walidacji\n",
    "krzyżowej\n",
    "* sprawdzić jakie są hiperparametry danego modelu i znaleźć optymalne\n",
    "* uczyć parametry modelu z danych, po skończonej nauce je wypisać"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
    "\n",
    "data = load_digits()\n",
    "\n",
    "X = data.data \n",
    "y = data.target \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # test_size -> 80% training and 20% test\n",
    "# random_state = 42 (służy jako ziarne losowości, aby wyniki były powtarzalne), natomiast jak wartość jest ta sama za każdym razem\n",
    "# to mamy ten sam podział\n",
    "\n",
    "print (\"\\nX_test = \", X_test.shape)\n",
    "print (\"Y_test = \", y_test.shape,)\n",
    "print (\"X_train = \", X_train.shape)\n",
    "print (\"Y_train = \", y_train.shape)\n",
    "\n",
    "# Walidacja krzyżowa k - fold\n",
    "# Dzielimy na k równych podzbiorów (w których każdy z podzbiorów raz występuje jako zbiór uczący, a pozostała, połączona \n",
    "# część zbioru jest wykorzystywana jako zbiór testowy)\n",
    "\n",
    "k = 5\n",
    "cv = StratifiedKFold(n_splits=k) # -> schemat walidacji krzyżowej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(X_train, X_test, Y_train, Y_test, model, param_grid, cv):\n",
    "    cross_val_scores = cross_val_score(model, X_train, Y_train, cv=cv, scoring='accuracy') # metryka oceny -> dokładność\n",
    "\n",
    "    # Walidacja krzyżowa - technika trenowania i testowania modelu na różnych podzbiorach danych\n",
    "    # Dostajemy procentowo jak dobrze model działa tzn jaka jest dokładność\n",
    "    # na danym podzbiorze danych\n",
    "    print(\"\\nWyniki walidacji krzyżowej: \\n\", cross_val_scores)\n",
    "    print(\"Średni wynik walidacji krzyżowej: \", cross_val_scores.mean()) \n",
    "\n",
    "    # Grid Search -> wyszukanie najlepszych hiperparametrów\n",
    "    # param grid -> określamy jakie hiperparametry chcemy przetestować\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=cv, scoring='accuracy')\n",
    "    grid_search.fit(X_train, Y_train)\n",
    "\n",
    "    # Najlepsze hiperparametry\n",
    "    best_params = grid_search.best_params_\n",
    "    print(f\"Najlepsze hiperparametry: {best_params}\")\n",
    "\n",
    "    # Trenowanie modelu z najlepszymi hiperparametrami\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_model.fit(X_train, Y_train)\n",
    "\n",
    "    # Predykcje i ocena na zbiorze testowym\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Dokładność na zbiorze testowym: {accuracy:.4f}\")\n",
    "    print(\"Raport klasyfikacji:\\n\", classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "    # Wypisanie końcowych parametrów modelu\n",
    "    print(\"Końcowe parametry modelu:\")\n",
    "    print(best_model.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Naiwny klasyfikator Bayesa\n",
    "Prosty, ale potężny algorytm uczenia maszynowego stosowany głównie do problemów klasyfikacji. Nazwa \"naiwny\" pochodzi od założenia, że wszystkie cechy (atrybuty) w zbiorze danych są niezależne od siebie, co rzadko jest prawdą w rzeczywistych zastosowaniach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianNB()\n",
    "param_grid = {\n",
    "    'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5]  # Dodaje niewielką wartość do wariancji, aby zapobiec dzieleniu przez zero\n",
    "}\n",
    "\n",
    "evaluate_model(X_train, X_test, y_train, y_test, model, param_grid, cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model kNN (model k najbliższych sąsiadów)\n",
    "Ma za zadnie znaleźć k sąsiadów, do których klasyfikowane obiekty są najbliższe dla wybranej metryki (np. Euklidesa), a następnie określa\n",
    "wynik klasyfikacji na podstawie większości głosów tych najbliższych k sąsiadów, biorąc pod uwagę, która klasa jest reprezentowana największą liczbę razy w grupie k najbliższych sąsiadów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier()\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9], # Liczba najbliższych sąsiadów\n",
    "    'weights': ['uniform', 'distance'], # Waga dla sąsiadów, uniform - wszyscy sąsiedzi mają taką samą wagę, distance - im bliżej tym większa waga\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'], # Algorytm używany do wyszukiwania najbliższych sąsiadów.\n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski'] # Metryka używana do obliczania odległości\n",
    "}\n",
    "\n",
    "# Poprawia wydajność modeli wrażliwych na skalowanie danych\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "evaluate_model(X_train, X_test, y_train, y_test, model, param_grid, cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Drzewo decyzyjne\n",
    "Model drzewa decyzyjnego jest strukturą drzewa, gdzie węzły reprezentują decyzje na podstawie wartości cech, a gałęzie przedstawiają możliwe wyniki tych decyzji. Liście drzewa reprezentują klasy (w przypadku klasyfikacji) lub wartości przewidywane (w przypadku regresji).\n",
    "\n",
    "#### Jak działa drzewo decyzyjne?\n",
    "##### Budowanie drzewa:\n",
    "\n",
    "- Algorytm zaczyna od całego zbioru danych jako korzenia drzewa.\n",
    "- W każdym węźle wybiera najlepszą cechę i wartość progu do podziału danych, maksymalizując czystość podzbiorów po podziale.\n",
    "- Proces ten jest rekurencyjnie powtarzany dla każdego podzbioru, tworząc gałęzie drzewa, aż do spełnienia kryterium zatrzymania (np. maksymalna głębokość drzewa, minimalna liczba próbek w liściu).\n",
    "Czystość podzbiorów:\n",
    "\n",
    "#### Czystość podzbiorów \n",
    "- Po podziale jest oceniana za pomocą miar takich jak indeks Giniego czy entropia. Celem jest maksymalizacja czystości podzbiorów, tj. dążenie do sytuacji, gdzie podzbiory zawierają próbki głównie jednej klasy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'], # Kryterium oceny jakości podziału. indeks Giniego/Entropia (używa entropii do oceny czystości podziału)\n",
    "    'max_depth': [None, 10, 20, 30], # Maksymalna głębokość drzewa, None - brak ograniczenia\n",
    "    'min_samples_split': [2, 5, 10], # Minimalna liczba próbek wymagana do podziału węzła\n",
    "    'min_samples_leaf': [1, 2, 4], # Minimalna liczba próbek wymagana w liściu węzła\n",
    "    'splitter': ['best', 'random'], # Strategia podziału węzła 'best' - najlepszy podział, 'random' - losowy podział\n",
    "    'max_features': ['sqrt', 'log2'], # Maksymalna liczba cech branych pod uwagę przy poszukiwaniu najlepszego podziału\n",
    "}\n",
    "\n",
    "evaluate_model(X_train, X_test, y_train, y_test, model, param_grid, cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Regresja logistyczna\n",
    "Algorytmem klasyfikacyjnym, który jest używany do przewidywania binarnych wyników (0 lub 1). Model ten oblicza prawdopodobieństwo przynależności próbki do określonej klasy, stosując funkcję logistyczną (sigmoidę)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(max_iter=1000) # max_iter - maksymalna liczba iteracji\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100], # Odwrotność siły regularyzacji, mniejsza wartość oznacza silniejszą regularyzację\n",
    "    'solver': ['liblinear', 'saga'], # Algorytm używany do optymalizacji\n",
    "    # 'penalty': ['l1', 'l2', 'elasticnet'], # Rodzaj regularyzacji (None - break regularyzacji)\n",
    "}\n",
    "\n",
    "# Poprawia wydajność modeli wrażliwych na skalowanie danych\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "evaluate_model(X_train, X_test, y_train, y_test, model, param_grid, cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Model LDA (Linear Discriminant Analysis)\n",
    "Jest to technika stosowana w statystyce, rozpoznawaniu wzorców i uczeniu maszynowym do znajdowania liniowej kombinacji cech, która najlepiej oddziela dwie lub więcej klas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearDiscriminantAnalysis()\n",
    "param_grid = [\n",
    "    {'solver': ['svd']},\n",
    "    {'solver': ['lsqr', 'eigen'], 'shrinkage': ['auto', 0.1, 0.5, 1.0]}\n",
    "]\n",
    "\n",
    "evaluate_model(X_train, X_test, y_train, y_test, model, param_grid, cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Model MLP (Prosta sieć neuronowa)\n",
    "Jest rodzajem sztucznej sieci neuronowej stosowanej w uczeniu nadzorowanym do rozwiązywania problemów klasyfikacyjnych. Jest to pełnoprawny klasyfikator, który uczy się na podstawie danych wejściowych i dostarcza predykcje klas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPClassifier(max_iter=100)\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 50)], # Wiele warstw ukrytych\n",
    "    'activation': ['tanh', 'relu'], # Funkcja aktywacji dla warstw ukrytych\n",
    "    'solver': ['sgd', 'adam'], # Optymalizator\n",
    "    'alpha': [0.0001, 0.1], # Parametr regularyzacji (L2)\n",
    "    'learning_rate': ['constant', 'adaptive'], # Plan uczenia\n",
    "}\n",
    "\n",
    "# Poprawia wydajność modeli wrażliwych na skalowanie danych\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "evaluate_model(X_train, X_test, y_train, y_test, model, param_grid, cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Model SVM (Support Vector Machine)\n",
    "Algorytm uczenia maszynowego używany głównie do zadań klasyfikacyjnych, ale może być również stosowany do regresji (Support Vector Regression). SVM znajduje hiperpłaszczyznę, która najlepiej rozdziela dane w przestrzeni cech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC()\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100], # Parametr regularyzacji. Kontroluje kompromis między maksymalizacją marginesu a minimalizacją błędu klasyfikacji.\n",
    "    'gamma': [1, 0.1, 0.01, 0.001], # Parametr funkcji jądrowej RBF (Radial Basis Function). \n",
    "    # Wyższa wartość oznacza większy wpływ próbek treningowych.\n",
    "    'kernel': ['linear', 'rbf'] # Funkcja jądrowa\n",
    "}\n",
    "\n",
    "# Poprawia wydajność modeli wrażliwych na skalowanie danych\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "evaluate_model(X_train, X_test, y_train, y_test, model, param_grid, cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Bagging\n",
    "Metoda zespołowa (ensemble), która poprawia stabilność i dokładność algorytmów uczenia maszynowego poprzez zmniejszenie wariancji. Metoda polega na trenowaniu wielu modeli bazowych na różnych losowo wybranych podzbiorach danych treningowych, a następnie łączeniu ich predykcji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = DecisionTreeClassifier()\n",
    "model = BaggingClassifier(estimator=base_model, n_estimators=100, random_state=42) # defaultowo estimator to DecisionTreeClassifier\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200], # Liczba modeli bazowych (drzew decyzyjnych) w ensemble.\n",
    "    'max_features': [0.5, 0.7, 1.0], # Maksymalna liczba cech branych pod uwagę przy poszukiwaniu najlepszego podziału\n",
    "    'max_samples': [0.5, 1.0], # Liczba próbek używanych do trenowania każdego modelu bazowego\n",
    "    'bootstrap': [True, False], # Czy próbki są losowane z powtórzeniami\n",
    "    'bootstrap_features': [True, False], # Czy cechy są losowane z powtórzeniami\n",
    "}\n",
    "evaluate_model(X_train, X_test, y_train, y_test, model, param_grid, cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Boosting\n",
    "Technika zespołowa, która tworzy silny model predykcyjny poprzez łączenie słabych modeli bazowych, zwykle drzew decyzyjnych. Metoda ta działa iteracyjnie, poprawiając błędy poprzednich modeli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingClassifier()\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200], # Liczba drzew decyzyjnych\n",
    "    'learning_rate': [0.01, 0.1, 1, 10], # Współczynnik uczenia\n",
    "    'max_depth': [3, 5, 7], # Maksymalna głębokość drzewa\n",
    "}\n",
    "evaluate_model(X_train, X_test, y_train, y_test, model, param_grid, cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Lasy losowe\n",
    "Metoda zespołowa (ensemble) polegająca na budowie wielu drzew decyzyjnych podczas treningu i wyjściu klasy będącej trybem klas (dla klasyfikacji) lub średnią predykcji (dla regresji) poszczególnych drzew. Random forest łączy prostotę drzew decyzyjnych z możliwościami agregacji w celu uzyskania lepszej wydajności i stabilności predykcji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(random_state=42)\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],  # Liczba drzew w lesie\n",
    "    'max_features': ['sqrt', 'log2'],  # Liczba cech do rozważenia przy podziale\n",
    "    'max_depth': [None, 10, 20, 30],  # Maksymalna głębokość drzew\n",
    "    'min_samples_split': [2, 5, 10],  # Minimalna liczba próbek potrzebnych do podziału węzła\n",
    "    'min_samples_leaf': [1, 2, 4],  # Minimalna liczba próbek w liściu\n",
    "    'bootstrap': [True, False]  # Czy losować próbki z powtórzeniami\n",
    "}\n",
    "evaluate_model(X_train, X_test, y_train, y_test, model, param_grid, cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uczenie nienadzorowane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inżynieria cech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selekcja cech (różne algorytmy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dane\n",
    "data = load_digits()\n",
    "\n",
    "X = data.data \n",
    "y = data.target \n",
    "\n",
    "# Trenowanie modelu\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standaryzacja danych\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Tworzenie modelu Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Trening modelu bez selekcji cech\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Dokładność na zbiorze testowym (bez selekcji): {accuracy:.4f}\")\n",
    "print(\"Raport klasyfikacji (bez selekcji):\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Selekcja cech na podstawie znaczenia cech\n",
    "selector = SelectFromModel(rf, prefit=True)\n",
    "X_train_rf = selector.transform(X_train)\n",
    "X_test_rf = selector.transform(X_test)\n",
    "\n",
    "# Trening modelu z selekcją cech (znaczenie cech)\n",
    "model.fit(X_train_rf, y_train)\n",
    "y_pred_rf = model.predict(X_test_rf)\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"Dokładność na zbiorze testowym (znaczenie cech): {accuracy_rf:.4f}\")\n",
    "print(\"Raport klasyfikacji (znaczenie cech):\\n\", classification_report(y_test, y_pred_rf))\n",
    "\n",
    "# Wybieranie najlepszych cech za pomocą testu ANOVA\n",
    "selector = SelectKBest(score_func=f_classif, k=2)\n",
    "X_train_best = selector.fit_transform(X_train, y_train)\n",
    "X_test_best = selector.transform(X_test)\n",
    "\n",
    "# Trenowanie modelu po selekcji cech\n",
    "model.fit(X_train_best, y_train)\n",
    "y_pred_best = model.predict(X_test_best)\n",
    "accuracy_best = accuracy_score(y_test, y_pred_best)\n",
    "print(f\"Dokładność na zbiorze testowym (Univariate Selection): {accuracy_best:.4f}\")\n",
    "print(\"Raport klasyfikacji (Univariate Selection):\\n\", classification_report(y_test, y_pred_best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redukcja wymiarowości: Metoda PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wczytywanie danych\n",
    "data = load_digits()\n",
    "\n",
    "X = data.data \n",
    "y = data.target \n",
    "\n",
    "# Podział danych na zbiór treningowy i testowy\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standaryzacja danych -> każda zmienna ma średnią 0 i wariancję 1 aby zmienne występujące w zbiorze danych były tej samej skali\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Trenowanie modelu bez redukcji wymiarowości\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Dokładność na zbiorze testowym (bez redukcji): {accuracy:.4f}\")\n",
    "print(\"Raport klasyfikacji (bez redukcji):\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Redukcja wymiarowości za pomocą PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "model.fit(X_train_pca, y_train)\n",
    "y_pred_pca = model.predict(X_test_pca)\n",
    "accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
    "print(f\"\\nDokładność na zbiorze testowym (PCA): {accuracy_pca:.4f}\")\n",
    "print(\"Raport klasyfikacji (PCA):\\n\", classification_report(y_test, y_pred_pca))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
